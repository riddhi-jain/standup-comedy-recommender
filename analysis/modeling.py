"""
Contains functions for performing topic modeling on vectorized comedy transcript corpus. If run as main script: creates
the topic model, extracts the top words from each topic for analysis, manually assigns topics, persists the model, and
makes some manual adjustments to the comedy special metadata.

Stephen Kaplan, 2020-08-13
"""
import pandas as pd
from sklearn.decomposition import NMF
import joblib

from insert_data_mongo import insert_to_mongo
from app.db import connect_to_mongo, load_mongo_collection_as_dataframe
from app.creds import USERNAME, PWD


def get_topics(model, n_components, vectorized_corpus):
    """


    :param model:
    :param n_components:
    :param vectorized_corpus:
    :return:
    """
    model = model(n_components)

    df_document_topic = pd.DataFrame(model.fit_transform(vectorized_corpus))
    df_topic_word = pd.DataFrame(model.components_, columns=vectorized_corpus.columns)
    df_word_topic = df_topic_word.transpose()

    return df_document_topic, df_topic_word, df_word_topic, topic_model


def get_top_words(df_word_topic, n_top_words=20):
    """
    Gets "top" words for each topic generated by a topic model. Helpful for interpreting what each topic represents and
    manually ascribing a name to each topic.

    :param pandas.DataFrame df_word_topic: Word-Topic matrix (rows are word and columns are topic weights.)
    :param int n_top_words: Number of top words to display for each topic.
    :return: DataFrame, each column representing a topic and containing the top words for that topic
    :rtype: pandas.DataFrame
    """
    df_top_words = pd.DataFrame()
    for col in df_word_topic.columns:
        df_top_words[col] = df_word_topic[col].sort_values(ascending=False).index[:n_top_words]

    return df_top_words


if __name__ == "__main__":
    # tried out LSA, LDA, and NMF with both word count and TF/IDF as input. NMF with TF/IDF seems best (for now)
    count_vectorized_comedy_corpus = pd.read_pickle('data/count_vectorized_standup_comedy_transcripts.pkl')
    tfidf_comedy_corpus = pd.read_pickle('data/tfidf_standup_comedy_transcripts.pkl')

    doc_topic, topic_word, word_topic, topic_model = get_topics(model=NMF, n_components=6,
                                                                vectorized_corpus=tfidf_comedy_corpus)

    # dump topic model to .pkl for use in search feature in flask app
    joblib.dump(topic_model, '../app/static/ml_models/tfidf_nmf_model.pkl')

    # use top words to decide on topic categories
    top_words = get_top_words(word_topic)
    topic_names = ['observational', 'theBlackExperience', 'britishAustralian',
                   'political', 'immigrantUpbringing', 'relationshipsSex']
    doc_topic.columns = topic_names

    # add topic weights to metadata and overwrite metadata collection in mongo db
    db = connect_to_mongo(username=USERNAME, password=PWD)
    metadata = load_mongo_collection_as_dataframe(db, 'metadata')
    metadata.drop(['comedyId', '_id'], axis=1, inplace=True)

    # fix final image URLs that were causing issues
    metadata.loc[2, 'imageUrl'] = "static/images/George Lopez - We'll Do It for Half.jpg"
    metadata.loc[193, 'imageUrl'] = "https://m.media-amazon.com/images/M/MV5BMjk0NjIwNTctMzk3ZC00OTYxLTg2NGEtNTU4OWY5MDQ3YmRlXkEyXkFqcGdeQXVyMTk3NDAwMzI@._V1_.jpg"

    # persist the updated metadata
    metadata2 = pd.merge(metadata, doc_topic, left_index=True, right_index=True)
    columns = ['comedyId', 'comedian', 'title', 'year', 'imageUrl']
    columns.extend(topic_names)
    insert_to_mongo(db, 'metadata', columns=columns, df=metadata2)
